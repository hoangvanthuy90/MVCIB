---
layout: post
title:  MVCIB, A Novel Pre-trained Graph Neural Network Model on 2D and 3D Molecular Structures
date:   2025-11-23 00:00:00 +0900
image:  Bib_Network.png
tags:   Release
published: true
description: The Network Science Lab at the Catholic University of Korea releases Multi-View Conditional Information Bottleneck, namely MVCIB, a novel Graph Transformer model specialised in 2D and 3D Molecular Structures.
---

We present [MVCIB](https://github.com/NSLab-CUK/Community-aware-Graph-Transformer), a novel architecture for pre-training Graph Neural Networks on 2D and 3D molecular structures and developed by NS Lab, CUK based on pure PyTorch backend.

<p align="center">
  <img src="/images/MVCIB.jpg" alt="Graph Transformer Architecture" width="800">
  <br>
  <b></b> The overall architecture of MVCIB.
</p>

We aim to build a pre-trained Graph Neural Network (GNN) model on 2D and 3D molecular structures. Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.


## A short description of MVCIB:

- Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart.
- To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views.
- Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability.
- Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.
 

## The MVCIB is available at:
* [![DOI](http://img.shields.io/:DOI-10.1109/TNSE.2025.3563697-FAB70C?style=flat-square&logo=doi)](https://doi.org/10.1109/TNSE.2025.3563697)
* [![GitHub](https://img.shields.io/badge/GitHub-Data%20&%20Code-9B9B9B?style=flat-square&logo=GitHub)](https://github.com/NSLab-CUK/Community-aware-Graph-Transformer)
* [![arXiv](https://img.shields.io/badge/arXiv-2504.15075-b31b1b?style=flat-square&logo=arxiv&logoColor=red)](https://arxiv.org/abs/2504.15075) 
[![arXiv](https://img.shields.io/badge/arXiv--Previous-2312.16788-b31b1b?style=flat-square&logo=arxiv&logoColor=red)](https://arxiv.org/abs/2312.16788)


## Cite "CGT" as: 

Please cite our [paper](https://arxiv.org/abs/2504.15075) if you find *CGT* useful in your work:
```
@misc{hoang2025mitigating,
      title={Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck}, 
      author={Van Thuy Hoang and O-Joun Lee},
      year={2025},
      eprint={2504.15075},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```

:page_facing_up::woman_technologist::bookmark_tabs::label::black_nib:	

## Contributors: 

<a href="https://github.com/NSLab-CUK/Unified-Graph-Transformer/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=NSLab-CUK/Unified-Graph-Transformer" />
</a>

***

<a href="https://nslab-cuk.github.io/"><img src="https://github.com/NSLab-CUK/NSLab-CUK/raw/main/Logo_Dual_Wide.png"/></a>

***


